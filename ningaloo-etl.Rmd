---
title: "Ningaloo ETL"
author: "Florian Mayer"
date: "4 July 2016"
output: html_document
---

# Install
Install required packages by running `install.R` once in your Ubuntu environment.
Note: root access is required to install the Ubuntu system package "mdbtools".
```{r, eval=FALSE}
source("install.R")
```

# Setup
Configure `ckanr` for use with our data catalogue.
The file `setup_ckanr.R` contains the confidential CKAN API key, which gives
the owner's write permissions: `ckanr::ckanr_setup(url=CKAN, key=APIKEY)`.

Create your own `setup_ckanr.R` from the template `setup_ckanr_template.R`.
Then, load required libraries and source `setup_ckanr.R`.
```{r, warning=FALSE, message=FALSE}
require(Hmisc)
require(dplyr)
require(lubridate)
require(ckanr)
require(DT)
require(rgdal)
require(mapview)
source("setup_ckanr.R")
```

# Extract data
Download, unzip and open the Access mdb file from the data catalogue.
```{r, echo=T}
tmp <- tempfile()
# download.file(resource_show(MDB_RID)$url, tmp)
# dbfile <- unzip(tmp, "ningaloov4.mdb")
dbfile <- "data/ningaloov4.mdb"
con <- mdb.get(dbfile, dateformat='%Y-%m-%d', as.is=T)
glimpse(con$tblDBAreaSurveyed)
```

# Transform

## Sites
Read `tblSections` (sites are called "subsections" here, walkable parts of a beach).
Drop sites with missing coordinates, and re-arrange SW and NE coordinates into 
five coordinate pairs (SW, SE, NE, NW, and SW again to close the rectangle).
This arrangement will make it easier for us to build bounding boxes later on. 
```{r}
goodsites <- con$tblSections %>%
  filter(!is.na(SubSect.NE.lat)) %>%
  transmute(
    id=as.numeric(SubSect.Id),
    subsection=as.character(txtSubSection),
    section=as.character(txtSections),
    division=as.character(division.name),
    center.lat=-as.numeric(SubSect.center.lat),
    center.lon=as.numeric(SubSect.center.long),
    sw.lon=as.numeric(SubSect.SW.long),
    sw.lat=-as.numeric(SubSect.SW.lat),
    se.lon=as.numeric(SubSect.NE.long),
    se.lat=-as.numeric(SubSect.SW.lat),
    ne.lon=as.numeric(SubSect.NE.long),
    ne.lat=-as.numeric(SubSect.NE.lat),
    ne.lon=as.numeric(SubSect.NE.long),
    ne.lat=-as.numeric(SubSect.NE.lat),
    nw.lon=as.numeric(SubSect.SW.long),
    nw.lat=-as.numeric(SubSect.NE.lat),
    end.lon=as.numeric(SubSect.SW.long),
    end.lat=-as.numeric(SubSect.SW.lat)
  )
row.names(goodsites) <- goodsites$id
```

On the other hand, sites with missing coordinates need to be fixed!
If there are any sites listed here, please supply the missing data, and upload
a new .zip archive of the original Access database to the data catalogue.
```{r}
badsites <- filter(con$tblSections, is.na(SubSect.NE.lat))
DT::datatable(badsites)
```


Following Stackoverflow user [jbaum](http://stackoverflow.com/users/489704/jbaums)'s [example](http://stackoverflow.com/a/26620550/2813717), we create the R equivalent
of a polygon shapefile.

`coords` is a matrix containing the five coordinate pairs (columns) for all sites (rows).
```{r}
coords <- goodsites %>%
  select(sw.lon, sw.lat, se.lon, se.lat, ne.lon, ne.lat, nw.lon, nw.lat, end.lon, end.lat) %>%
  as.matrix()
```

`ids` is a vector containing the site IDs.
```{r}
ids <- goodsites %>% select(id) %>% as.matrix()
```

`make_polygons` creates one `Polygon` object from a matrix of coordinates and a 
vector of IDs like the above.
```{r}
#' Create SpatialPolygons from a vector of lon/lat coordinates (poly) and IDs
make_polygons <- function(poly, id) {
  Polygons(list(Polygon(matrix(poly, ncol=2, byrow=TRUE))), ID=id)
}
```

Create SpatialPolygons (SpatialPolygons `polys`) from the coordinates (matrix `coords`) and IDs (matrix `ids`)
by vectorising the function `make_polygons` over all sites (one site is one row 
in `coords`) using the correct projection (WGS84).
```{r}
# 
wgs84 <- CRS("+proj=longlat +ellps=WGS84 +datum=WGS84")
polys <- SpatialPolygons(mapply(make_polygons, split(coords, row(coords)), ids), proj4string=wgs84)
```

Create a SpatialPolygonsDataFrame `polys.df` from SpatialPolygons `polys` and a 
data.frame `goodsites`. This is the R version of a shapefile, containing spatial
features (`polys`) as well as text attributes (`goodsites`).
```{r}
polys.df <- SpatialPolygonsDataFrame(polys, as.data.frame(goodsites))
```

Write the SPDF to GeoJSON and preview them on an interactive map.
```{r}
writeOGR(polys.df, "data/sites.geojson", layer="geojson", driver="GeoJSON")
mapview(polys.df)
```

## Surveys
Surveys are joined to environmental condition, column names are sanitised.
Read `tblDBAreaSurveyed`, parse date formats, infer timezone GMT+08, resolve
lookup for Yes/No, and clean up column names.
```{r}
# Surveys
surveys <- con$tblDBAreaSurveyed %>%
  left_join(con$tblEnvironCond, by="date.id") %>%
  left_join(con$tblYsnDisturbed, by="Ysn.id") %>%
  mutate(date.id=parse_date_time(date.id, orders=ord, tz=tz)) %>%
  rename(date=date.id,
         disturbed=txt.YesNoDist,
         survey_id=area.svyd.id,
         division=Divsion,
         section=Section,
         subsection=SubSection,
         no_false_crawls_fox_tracks=numFalseCrawlsFoxTracks,
         fox_tracks_present=FoxTracks,
         dog_tracks_present=DogTracks,
         wind_speed=wind.speed,
         wind_direction=wind.direction,
         air_temp=air.temp,
         water_temp=water.temp,
         time_of_high_tide=time.HT,
         height_of_high_tide=hght.HT) %>%
  left_join(goodsites, by="subsection") %>%
  select(-starts_with("Ysn.id"))
write.csv(surveys, file = "data/surveys.csv")
DT::datatable(surveys)
```


## Lookups
Let's clean up the column names of lookups.
```{r}
species <- rename(con$tblTurtleSpecies, species_id=Turtle.Species.ID, species_name=Turtle.Species.Name)
nest_types <- rename(con$tblNestType, nest_type_id=NestTypeID, nest_type=NestType)
confidence <- rename(con$tblPosConf, confidence_id=PosConf.ID, confidence=txtPosConf)
position <-  rename(con$tblProfilePos, position_id=intPosID, position=Position)
yes_no <- rename(con$tblYsnDisturbed, yes_no_id=Ysn.id, yes_no=txt.YesNoDist)
track_type <- rename(con$tblTrackType, track_id=track.id, track_origin=txtTrackName)
```

## Observations: False crawls
Munge `tblDBFalseCrawl`:

* Clean up column names, 
* resolve species lookups, 
* append survey level data, 
* write to CSV and preview.
```{r}
crawls <- con$tblDBFalseCrawl %>%
  rename(species_id=FalseCrawlSpecies,
         survey_id=area.svyd.id,
         no_false_crawls=NumberFalseCrawls,
         crawl_id=FalseCrawlRecordID) %>%
  left_join(species, by="species_id") %>%
  left_join(surveys, by="survey_id")
write.csv(crawls, file = "data/crawls.csv")
DT::datatable(crawls)
```

## Observations: Real nests
Like with false crawls, munge `tblDBNestingSurvey`:

* sanitise column names, 
* resolve all lookups and drop their purely internal IDs, 
* restore the missing negative latitude sign, 
* append survey level data, 
* save to CSV and preview.
```{r}
nests <- con$tblDBNestingSurvey %>%
  rename(nest_id=NestID,
         survey_id=area.svyd.id,
         nest_type_id=NestType,
         confidence_id=PosConf.ID,
         position_id=intPosID,
         yes_no_id=ysnNestDist.ID,
         crawl_id=crawl.id,
         track_id=track.id,
         track_id2=track.id2,
         camera_photo_no=CameraPhotoNo,
         comments=Comments) %>%
  left_join(nest_types, by="nest_type_id") %>%
  left_join(confidence, by="confidence_id") %>%
  left_join(position, by="position_id") %>%
  left_join(yes_no, by="yes_no_id") %>%
  left_join(track_type, by="track_id") %>%
  left_join(surveys, by="survey_id") %>%
  rename(disturbed_nest=yes_no) %>%
  mutate(latitude=-as.numeric(latitude),
         longitude=as.numeric(longitude)) %>%
  select(-nest_type_id, -confidence_id, -position_id, -yes_no_id)
write.csv(nests, file = "data/nests.csv")
DT::datatable(nests)
```

# Load
Upload outputs to CKAN.
```{r}
# ckanr::resource_update(ETL_RID, "ningaloo-etl.html")
# ckanr::resource_update(SITES_RID, path="data/sites.geojson")
# ckanr::resource_update(CRAWL_RID, "data/crawls.csv")
# ckanr::resource_update(NEST_RID, "data/nests.csv")
```

# Cleanup
Close the temporary file.
```{r}
unlink(tmp)
```
