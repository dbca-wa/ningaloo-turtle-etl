---
title: "Ningaloo Turtle Program Data Extraction, Transformation and Loading"
author: "Florian Mayer"
date: "4 July 2016"
output: html_document
---
This workbook executes and documents the extraction, transformation and loading
(ETL) of Ningaloo Turtle Program data from the original Access database into
accessible formats. Data are read from a snapshot of the original database on the internal data catalogue, processed locally, and uploaded as various outputs 
back to the data catalogue.

**Note** The data shown are not yet fully quality controlled, and therefore could contain duplication or gaps. This notebook currently is a work in progress.

# Install
Install required packages by running `install.R` once in your Ubuntu environment.
Note: root access is required to install the Ubuntu system package "mdbtools".
```{r, eval=FALSE}
source("install.R")
```

# Setup
Configure `ckanr` for use with our data catalogue.
The file `setup_ckanr.R` contains the confidential CKAN API key, which gives
the owner's write permissions: `ckanr::ckanr_setup(url=CKAN, key=APIKEY)`.

Create your own `setup_ckanr.R` from the template `setup_ckanr_template.R`.
Then, load required libraries and source `setup_ckanr.R`.
```{r, warning=FALSE, message=FALSE}
require(Hmisc)
require(dplyr)
require(lubridate)
require(ckanr)
require(DT)
require(rgdal)
require(mapview)
source("setup_ckanr.R")
```

# Extract data
Download, unzip and open the Access mdb file from the data catalogue.
```{r, echo=T, message=FALSE}
tmp <- tempfile(tmpdir="data")
download.file(resource_show(MDB_RID)$url, tmp)
dbfile <- unzip(tmp, "ningaloov4.mdb")
# dbfile <- "data/ningaloov4.mdb"
con <- mdb.get(dbfile, dateformat='%Y-%m-%d', as.is=T)
contents(con)
```

# Transform

## Sites
In `tblSections`, sites are called "subsections", representing walkable parts 
of a beach.
Where all coordinates are given, we'll permutate SW and NE coordinates into 
five coordinate pairs (SW, SE, NE, NW, and SW again to close the rectangle).
This arrangement will make it easier for us to build bounding boxes later on.
```{r}
sites <- con$tblSections %>%
#   filter(!is.na(SubSect.NE.lat) & !is.na(SubSect.NE.long) &
#     !is.na(SubSect.SW.lat) & !is.na(SubSect.SW.long)) %>%
  transmute(
    id=as.numeric(SubSect.Id),
    subsection=as.character(txtSubSection),
    section=as.character(txtSections),
    division=as.character(division.name),
    center.lat=-as.numeric(SubSect.center.lat),
    center.lon=as.numeric(SubSect.center.long),
    sw.lon=as.numeric(SubSect.SW.long),
    sw.lat=-as.numeric(SubSect.SW.lat),
    se.lon=as.numeric(SubSect.NE.long),
    se.lat=-as.numeric(SubSect.SW.lat),
    ne.lon=as.numeric(SubSect.NE.long),
    ne.lat=-as.numeric(SubSect.NE.lat),
    ne.lon=as.numeric(SubSect.NE.long),
    ne.lat=-as.numeric(SubSect.NE.lat),
    nw.lon=as.numeric(SubSect.SW.long),
    nw.lat=-as.numeric(SubSect.NE.lat),
    end.lon=as.numeric(SubSect.SW.long),
    end.lat=-as.numeric(SubSect.SW.lat)
  )
row.names(sites) <- sites$id

# Manually fix mising NE corner of Red Bluff
redbluffrow <- which(sites$subsection=="Red Bluff")
sites[redbluffrow,]$ne.lon <- 113.458
sites[redbluffrow,]$se.lon <- 113.458
sites[redbluffrow,]$ne.lat <- -24.016
sites[redbluffrow,]$nw.lat <- -24.016
```

### Sites with missing coordinates
Although we can patch the data here, sites with missing coordinates need to be fixed in the original data source!
If there are any sites listed here, please supply the missing data in the original
NTP access database, then zip and upload the original Access database to the data catalogue.
```{r}
badsites <- filter(con$tblSections, 
  is.na(SubSect.NE.lat) | is.na(SubSect.NE.long) | 
    is.na(SubSect.SW.lat) | is.na(SubSect.SW.long))
DT::datatable(badsites, caption="Sites with missing coordinates")
```

### Shapefiles - R style
Following Stackoverflow user [jbaum](http://stackoverflow.com/users/489704/jbaums)'s [example](http://stackoverflow.com/a/26620550/2813717), we create the R equivalent
of a polygon shapefile.

`coords` is a matrix containing the five coordinate pairs (columns) for all sites (rows).
```{r}
coords <- sites %>%
  select(sw.lon, sw.lat, 
         se.lon, se.lat, 
         ne.lon, ne.lat, 
         nw.lon, nw.lat, 
         end.lon, end.lat) %>%
  as.matrix()
```

`ids` is a vector containing the site IDs.
```{r}
ids <- sites %>% select(id) %>% as.matrix()
```

`make_polygons` creates one `Polygon` object from a matrix of coordinates and a 
vector of IDs like the above.
```{r}
#' Create SpatialPolygons from a vector of lon/lat coordinates (poly) and IDs
make_polygons <- function(poly, id) {
  Polygons(list(Polygon(matrix(poly, ncol=2, byrow=TRUE))), ID=id)}
```

Create SpatialPolygons (SpatialPolygons `polys`) from the coordinates (matrix `coords`) and IDs (matrix `ids`)
by vectorising the function `make_polygons` over all sites (one site is one row 
in `coords`) using the correct projection (WGS84).
```{r}
wgs84 <- CRS("+proj=longlat +ellps=WGS84 +datum=WGS84")
polys <- SpatialPolygons(
  mapply(make_polygons, split(coords, row(coords)), ids), proj4string=wgs84)
```

Create a SpatialPolygonsDataFrame `polys.df` from SpatialPolygons `polys` and a 
data.frame `sites`. This is the R version of a shapefile, containing spatial
features (`polys`) as well as text attributes (`sites`).
```{r}
polys.df <- SpatialPolygonsDataFrame(polys, as.data.frame(sites))
```

Write the SPDF to GeoJSON and preview them on an interactive map.
```{r}
writeOGR(polys.df, "data/sites.geojson", layer="geojson", driver="GeoJSON")
mapview(polys.df)
```

## Surveys
Surveys are joined to environmental condition, column names are sanitised.
Read `tblDBAreaSurveyed`, parse date formats, infer timezone GMT+08, resolve
lookup for Yes/No, and clean up column names.
```{r}
ord <- c("mdyHMS")
tz <- "GMT"
surveys <- con$tblDBAreaSurveyed %>%
  left_join(con$tblEnvironCond, by="date.id") %>%
  left_join(con$tblYsnDisturbed, by="Ysn.id") %>%
  mutate(date.id=parse_date_time(date.id, orders=ord, tz=tz)) %>%
  rename(date=date.id,
         disturbed=txt.YesNoDist,
         survey_id=area.svyd.id,
         division=Divsion,
         section=Section,
         subsection=SubSection,
         no_false_crawls_fox_tracks=numFalseCrawlsFoxTracks,
         fox_tracks_present=FoxTracks,
         dog_tracks_present=DogTracks,
         wind_speed=wind.speed,
         wind_direction=wind.direction,
         air_temp=air.temp,
         water_temp=water.temp,
         time_of_high_tide=time.HT,
         height_of_high_tide=hght.HT) %>%
  select(-division, -section) %>%
  left_join(sites, by="subsection") %>%
  select(-starts_with("Ysn.id"))
write.csv(surveys, file = "data/surveys.csv", row.names=F)
# DT::datatable(surveys)

duplicated_surveys <- surveys %>% filter(duplicated(surveys$survey_id))
# DT::datatable(duplicated_surveys, caption = "Duplicated survey IDs")
```


## Lookups
Let's clean up the column names of lookups.
```{r}
species <- rename(con$tblTurtleSpecies, species_id=Turtle.Species.ID, species_name=Turtle.Species.Name)
nest_types <- rename(con$tblNestType, nest_type_id=NestTypeID, nest_type=NestType)
confidence <- rename(con$tblPosConf, confidence_id=PosConf.ID, confidence=txtPosConf)
position <-  rename(con$tblProfilePos, position_id=intPosID, position=Position)
yes_no <- rename(con$tblYsnDisturbed, yes_no_id=Ysn.id, yes_no=txt.YesNoDist)
track_type <- rename(con$tblTrackType, track_id=track.id, track_origin=txtTrackName)
```

## Observations: False crawls
Munge `tblDBFalseCrawl`:

* Clean up column names, 
* drop duplicated "division" and "section",
* resolve species lookups, 
* append survey level data, 
* write to CSV and preview.

**QA check** 13577 observations, after joining surveys: 13798
some crawl_ids are duplicated

```{r}
crawls <- con$tblDBFalseCrawl %>%
  rename(species_id=FalseCrawlSpecies,
         survey_id=area.svyd.id,
         no_false_crawls=NumberFalseCrawls,
         crawl_id=FalseCrawlRecordID) %>%
  left_join(species, by="species_id") %>%
  select(-species_id) %>%
  left_join(surveys, by="survey_id")
write.csv(crawls, file = "data/crawls.csv", row.names=F)
# DT::datatable(crawls)

duplicated_crawls <- crawls %>% filter(duplicated(crawls$crawl_id))
# DT::datatable(duplicated_crawls, caption = "Duplicated crawls")
```

## Observations: Real nests
Like with false crawls, munge `tblDBNestingSurvey`:

* sanitise column names, 
* resolve all lookups and drop their purely internal IDs, 
* restore the missing negative latitude sign, 
* append survey level data, 
* save to CSV and preview.

```{r}
nests <- con$tblDBNestingSurvey %>%
  rename(nest_id=NestID,
         survey_id=area.svyd.id,
         nest_type_id=NestType,
         confidence_id=PosConf.ID,
         position_id=intPosID,
         yes_no_id=ysnNestDist.ID,
         species_id=crawl.id,
         track_id=track.id,
         track_id2=track.id2,
         camera_photo_no=CameraPhotoNo,
         comments=Comments) %>%
  left_join(nest_types, by="nest_type_id") %>%
  left_join(confidence, by="confidence_id") %>%
  left_join(position, by="position_id") %>%
  left_join(yes_no, by="yes_no_id") %>%
  left_join(track_type, by="track_id") %>%
  left_join(species, by="species_id") %>%
  left_join(surveys, by="survey_id") %>%
  rename(disturbed_nest=yes_no) %>%
  mutate(latitude=-as.numeric(latitude),
         longitude=as.numeric(longitude)) %>%
  select(-nest_type_id, -confidence_id, -position_id, 
         -yes_no_id, -species_id, -track_id)
write.csv(nests, file = "data/nests.csv", row.names=F)
# DT::datatable(nests)

duplicated_nests <- nests %>% filter(duplicated(nests$nest_id))
# DT::datatable(duplicated_crawls, caption = "Duplicated crawls")
```

### Summary of new nests by survey
The sum of new nests for every unique combination of survey (subsection, date)
and species.
```{r}
summary_nests <- tbl_df(nests) %>%
  filter(nest_type=="New") %>%
  group_by(subsection, date, species_name) %>%
  tally(sort=T) %>%
  ungroup() %>%
  left_join(surveys)
write.csv(summary_nests, file = "data/summary_nests.csv", row.names=F)
DT::datatable(summary_nests)
```

### False crawls by survey
The sum of false crawls for every unique combination of survey (subsection, date)
and species.
```{r}
summary_crawls <- tbl_df(crawls) %>%
  group_by(subsection, date, species_name) %>%
  tally(sort=T) %>%
  ungroup() %>%
  left_join(surveys)
write.csv(summary_crawls, file = "data/summary_crawls.csv", row.names=F)
# DT::datatable(summary_crawls)
```

# Load
Upload outputs to CKAN.
```{r}
ckanr::resource_update(ETL_RID, "ningaloo-etl.html")
ckanr::resource_update(SITES_RID, path="data/sites.geojson")
ckanr::resource_update(SURVEYS_RID, path="data/surveys.csv")
ckanr::resource_update(CRAWL_RID, "data/crawls.csv")
ckanr::resource_update(NEST_RID, "data/nests.csv")
ckanr::resource_update(NEW_NEST_RID, "data/summary_nests.csv")
ckanr::resource_update(FALSE_CRAWL_RID, "data/summary_crawls.csv")
```

# Cleanup
Close the temporary file.
```{r}
unlink(tmp)
```
